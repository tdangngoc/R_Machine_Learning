---
title: "R_Markdown"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 1 : Exercise

```{r}
cost_data = read.csv("costpercompany.csv", header = TRUE)
cost_data
```

```{r}
# Statistical eploration of dataset
names(cost_data)
head(cost_data)
tail(cost_data)
str(cost_data)
summary(cost_data)
```

```{r}
# Dimension of dataset
dim(cost_data)
```

```{r}
# Install package for cluster analysis
install.packages("cluster")
library(cluster)
```

```{r}
pairs(cost_data)
```

```{r}
plot(Fcost ~ Sales, data = cost_data)
with(cost_data, text(Fcost ~ Sales, labels = Company, pos = 4, cex = .6))
```

```{r}
# Function to normalize dataset to get the same scale
normalize = function(df)
{
  return(((df-min(df))/ (max(df)-min(df))*(1-0))+0)
}
```

```{r}
head(cost_data)
```

```{r}
company = cost_data[,1]
cost_data_n = cost_data[,2:9] # Remove company column to normalise
cost_data_n = as.data.frame(lapply(cost_data_n, normalize))
cost_data_n$Company = company # Add company column after normalise
cost_data_n = cost_data_n[,c(9,1,2,3,4,5,6,7,8)]
```

```{r}
head(cost_data_n)
```

```{r}
#Create distance matrix
distance = dist(cost_data_n, method = "euclidean")
distance
```

```{r}
# Visualise distance matrix using factoextra package
install.packages("factoextra")
library(factoextra)
```

```{r}
fviz_dist(distance)
```

```{r}
row.names(cost_data_n) = cost_data_n$Company
cost_data_n$Company <- NULL
head(cost_data_n)
```

```{r}
distance = dist(cost_data_n, method = "euclidean")
fviz_dist(distance)
```

## Hierarchical Clustering

```{r}
cost_data_hclust <- hclust(distance)
cost_data_hclust
plot(cost_data_hclust)
plot(cost_data_hclust, hang = -1)
plot(cost_data_hclust, labels = cost_data$Company)
```

```{r}
plot(cost_data_hclust, labels = cost_data$Company)
rect.hclust(cost_data_hclust, 3)
```

```{r}
plot(cost_data_hclust, labels = cost_data$Company)
rect.hclust(cost_data_hclust, 4)
```

```{r}
#Avrage Linkage Clustering
hclust_average = hclust(distance, method = "average")
plot(hclust_average, labels = cost_data$Company)
rect.hclust(hclust_average, 4)
```

```{r}
# Single Linkage Clustering
hclust_single <- hclust(distance, method = "single")
plot(hclust_single, labels = cost_data$Company)
rect.hclust(hclust_single, 4)
```

```{r}
# Centroid Linkage Clustering
hclust_centroid <- hclust(distance, method="centroid")
plot(hclust_centroid, labels = cost_data$Company)
rect.hclust(hclust_centroid, 4)
```

```{r}
hclust_complete <- hclust(distance, method = "complete")
plot(hclust_complete, labels = cost_data$Company)
rect.hclust(hclust_complete, 4)
```

```{r}
member_centroid <- cutree(hclust_centroid, 4)
member_centroid

member_complete <- cutree(hclust_complete, 4)
member_complete

table(member_centroid, member_complete)
```

# K-Means

```{r}
kc <- kmeans(cost_data[,-1], 3)
kc
```

```{r}
#Plotting cluster

clusplot(cost_data, kc$cluster, color = TRUE, shade = TRUE, lines = 0)
```

# Exercise

```{r}
crimes <- read.csv("crimes-2017-18.csv", header = TRUE)
```

```{r}
names(crimes)
head(crimes)
tail(crimes)
str(crimes)
summary(crimes)
```

```{r}
dim(crimes)
```

```{r}
crimes <- crimes[,c(3,7,9)]
head(crimes)
```

```{r}
# Install package reshape2 to transform data between wide and long formats
install.packages("reshape2")
library(reshape2)
```

```{r}
crimes_pivot <- dcast(crimes, Force.Name ~ Offence.Subgroup, sum, value.var = "Number.of.offences")
crimes_pivot
```

```{r}
rownames(crimes_pivot) <- crimes_pivot$Force.Name
crimes_pivot[,1] <- NULL
head(crimes_pivot)
```

```{r}
# Normalise dataset
normalize <- function(df)
{
  return(((df-min(df))/(max(df)-min(df))*(1-0)) + 0)
}
```

```{r}
Force.Name <- rownames(crimes_pivot)
crimes_pivot_n <- as.data.frame(lapply(crimes_pivot, normalize))
rownames(crimes_pivot_n) <- Force.Name

head(crimes_pivot_n)
```

### Assessing clustering tendency

```{r}
tendency <- get_clust_tendency(crimes_pivot_n, 30, graph = TRUE)
tendency$hopkins_stat
```

```{r}
fviz_nbclust(crimes_pivot_n, kmeans, method = "wss")
```

```{r}
set.seed(123)
km_fit <- kmeans(crimes_pivot_n, 3, nstart = 30)
km_fit$cluster
km_fit$size
```

```{r}
fviz_cluster(km_fit, crimes_pivot_n)
```

```{r}
# Remove the outlier Metropolitan Police
crimes_pivot_n2 <- subset(crimes_pivot_n, rownames(crimes_pivot_n) != "Metropolitan Police")

km_fit <- kmeans(crimes_pivot_n2, 3, nstart = 30)
km_fit$cluster
km_fit$size
```

```{r}
fviz_cluster(km_fit, crimes_pivot_n2)
```

```{r}
crimes_pivot_n3 <- subset(crimes_pivot_n, !rownames(crimes_pivot_n) %in% c("Metropolitan Police", "Greater Manchester", "West Yorkshire", "West Midlands"))
```

```{r}
km_fit <- kmeans(crimes_pivot_n3, 3, nstart = 30)
km_fit$cluster
km_fit$size
```

```{r}
fviz_cluster(km_fit, crimes_pivot_n3, ellipse.type = "norm")
```

